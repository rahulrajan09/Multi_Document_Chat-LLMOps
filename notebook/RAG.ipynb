{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "! uv pip install langchain-openai tiktoken rapidocr-onnxruntime python-dotenv langchain-community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85eda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install langchain-ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama,OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef818c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nollama can be used here if needed\\n\\nmodel=ChatOllama(model=\"qwen3:1.7b\",validate_model_on_init=True)\\nprint(model.invoke(\"explain NPU in 5 lines\"))\\n\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the llm model with its parameters\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_API= os.getenv(\"OPENROUTER_MODEL_KEY\")\n",
    "MODEL_NAME= os.getenv(\"OPENROUTER_MODEL_NAME\")\n",
    "MODEL_url= os.getenv(\"base_url\")\n",
    "\n",
    "\n",
    "model=ChatOpenAI(model=MODEL_NAME,\n",
    "                 api_key=MODEL_API,\n",
    "                 base_url=MODEL_url,\n",
    "                 )\n",
    "\n",
    "'''\n",
    "ollama can be used here if needed\n",
    "\n",
    "model=ChatOllama(model=\"qwen3:1.7b\",validate_model_on_init=True)\n",
    "print(model.invoke(\"explain NPU in 5 lines\"))\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4974fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Neural Processing Unit (NPU)** – a specialized hardware accelerator designed to run AI workloads efficiently.  \n",
      "1️⃣ Optimized for matrix‑multiply‑accumulate (MAC) operations that dominate deep‑learning inference and training.  \n",
      "2️⃣ Uses low‑precision (e.g., 8‑bit, 4‑bit) arithmetic and parallel systolic arrays to boost throughput while cutting power.  \n",
      "3️⃣ Integrated into smartphones, edge devices, and servers to offload neural‑net computations from CPUs/GPGPUs.  \n",
      "4️⃣ Provides deterministic latency, making it ideal for real‑time vision, speech, and sensor‑fusion applications.\n"
     ]
    }
   ],
   "source": [
    "#checking llm model working\n",
    "messages = [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a QA assistant helping people to grasp the oncepts asked about.\",\n",
    "            ),\n",
    "            (\"human\", \"explain NPU in 5 lines\"),\n",
    "        ]\n",
    "msg=model.invoke(messages)\n",
    "print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d4a75",
   "metadata": {},
   "source": [
    "### DATA INGESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6f8ce",
   "metadata": {},
   "source": [
    "#### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb2819a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pdf file using file path stored in .env file and parsing it using pdfloader\n",
    "pdf_file_path =os.getenv(\"PDF_PATH\")\n",
    "loader=PyPDFLoader(file_path=pdf_file_path,extract_images=True)\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5904611a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#book has 135 pages accurately divided it as per that\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25d89de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed intro-preface and last acknowledgement page since its of no use\n",
    "documents=documents[2:134]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5984277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9e36c",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1cb34071",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=RecursiveCharacterTextSplitter(chunk_size=900,chunk_overlap=90)\n",
    "text_chunks=splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a18ffcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ce117b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "1.1 What is Machine Learning\n",
      "Machine learning is a subﬁeld of computer science that is concerned with building algorithms\n",
      "which, to be useful, rely on a collection of examples of some phenomenon. These examples\n",
      "can come from nature, be handcrafted by humans or generated by another algorithm.\n",
      "Machine learning can also be deﬁned as the process of solving a practical problem by 1)\n",
      "gathering a dataset, and 2) algorithmically building a statistical model based on that dataset.\n",
      "That statistical model is assumed to be used somehow to solve the practical problem.\n",
      "To save keystrokes, I use the terms “learning” and “machine learning” interchangeably.\n",
      "1.2 Types of Learning\n",
      "Learning can be supervised, semi-supervised, unsupervised and reinforcement.\n",
      "1.2.1 Supervised Learning\n",
      "In supervised learning1, thedataset is the collection oflabeled examples{(xi, yi)}N\n",
      "i=1.\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9b07f",
   "metadata": {},
   "source": [
    "#### Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9d6e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling embedding model\n",
    "embeddings=OllamaEmbeddings(model=\"embeddinggemma:latest\",\n",
    "                           validate_model_on_init=True,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7f608",
   "metadata": {},
   "source": [
    "#### Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a19ebb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide the list of documents to embed along with the embedding fucntion which needs to be performed on text\n",
    "vectorstore=FAISS.from_documents(text_chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54c342",
   "metadata": {},
   "source": [
    "# Perform similarity search\n",
    "query = \"explain gradient descent?\"\n",
    "docs = vectorstore.similarity_search(query, k=4)\n",
    "\n",
    "# Display the results\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaad9d2",
   "metadata": {},
   "source": [
    "#### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0de3bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045360f",
   "metadata": {},
   "source": [
    "### Query from user side------embedding-------retriver will fetch context----prompt-passing query and context retrived from retriver-----passto llm---response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c9131",
   "metadata": {},
   "source": [
    "- always decide input i.e query\n",
    "- output -i.e stroutputparser\n",
    "- llm calling model\n",
    "- chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae52895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use ten sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a969293",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8d6d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "da3b77bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain\n",
    "rag_chain=(\n",
    "    {\"context\":retriver,\"question\":RunnablePassthrough()}\n",
    "    |prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2343d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=rag_chain.invoke(\"explain normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0d65632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization is a preprocessing step that rescales a numeric feature to a fixed, small interval—most commonly [0, 1] or [–1, 1].  \n",
      "For each value x(j) of feature j, the normalized value \\(\\bar{x}(j)\\) is computed as  \n",
      "\n",
      "\\[\n",
      "\\bar{x}(j)=\\frac{x(j)-\\min(j)}{\\max(j)-\\min(j)},\n",
      "\\]\n",
      "\n",
      "where \\(\\min(j)\\) and \\(\\max(j)\\) are the smallest and largest observed values of that feature in the dataset.  \n",
      "By subtracting the minimum and dividing by the range, all transformed values fall inside the chosen interval.  \n",
      "Normalization helps gradient‑based learning algorithms converge faster because no single feature can dominate the updates due to a larger scale.  \n",
      "It also reduces numerical issues that arise when computers handle very large or very small numbers.  \n",
      "The technique is especially useful when features have different units or ranges, such as one ranging from 0‑1000 and another from 0‑0.0001.  \n",
      "While not strictly required, most modern machine‑learning pipelines apply normalization (or a similar scaling) as a default step.  \n",
      "If the data are roughly normally distributed or contain outliers, standardization (z‑score scaling) may be preferred, but normalization remains the go‑to method for bringing all features into a common, bounded range.\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b67223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MultiDocRAG-LLMOPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

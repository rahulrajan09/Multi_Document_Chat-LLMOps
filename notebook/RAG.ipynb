{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d80c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! uv pip install langchain-openai tiktoken rapidocr-onnxruntime python-dotenv langchain-community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac818cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\AI RAG Projects\\MultiDocRAG-LLMOPS\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e85eda7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.12 environment at: D:\\AI RAG Projects\\MultiDocRAG-LLMOPS\\.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 25ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install langchain-ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4ed5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama,OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef818c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nollama can be used here if needed\\n\\nmodel=ChatOllama(model=\"qwen3:1.7b\",validate_model_on_init=True)\\nprint(model.invoke(\"explain NPU in 5 lines\"))\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "MODEL_API= os.getenv(\"OPENROUTER_MODEL_KEY\")\n",
    "MODEL_NAME= os.getenv(\"OPENROUTER_MODEL_NAME\")\n",
    "MODEL_url= os.getenv(\"base_url\")\n",
    "\n",
    "\n",
    "model=ChatOpenAI(model=MODEL_NAME,\n",
    "                 api_key=MODEL_API,\n",
    "                 base_url=MODEL_url,\n",
    "                 )\n",
    "\n",
    "'''\n",
    "ollama can be used here if needed\n",
    "\n",
    "model=ChatOllama(model=\"qwen3:1.7b\",validate_model_on_init=True)\n",
    "print(model.invoke(\"explain NPU in 5 lines\"))\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d4974fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Neural Processing Unit (NPU)** – a specialized hardware accelerator designed to run deep‑learning inference and training workloads far more efficiently than general‑purpose CPUs or GPUs.  \n",
      "It contains a massive array of MAC (multiply‑accumulate) units and on‑chip memory that execute tensor operations (e.g., convolutions, matrix multiplies) in parallel with minimal data movement.  \n",
      "NPUs use low‑precision arithmetic (often 8‑bit or mixed‑precision) to boost throughput while keeping power consumption low, making them ideal for edge devices and mobile AI.  \n",
      "Typical applications include image/video recognition, natural‑language processing, recommendation systems, and autonomous‑driving perception pipelines.  \n",
      "By offloading AI tasks to an NPU, systems achieve higher performance per watt, reduced latency, and longer battery life compared with CPU‑only solutions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a QA assistant helping people to grasp the oncepts asked about.\",\n",
    "            ),\n",
    "            (\"human\", \"explain NPU in 5 lines\"),\n",
    "        ]\n",
    "msg=model.invoke(messages)\n",
    "print(msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d4a75",
   "metadata": {},
   "source": [
    "### DATA INGESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca6f8ce",
   "metadata": {},
   "source": [
    "#### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb2819a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pdf file using file path stored in .env file and parsing it using pdfloader\n",
    "pdf_file_path =os.getenv(\"PDF_PATH\")\n",
    "loader=PyPDFLoader(file_path=pdf_file_path,extract_images=True)\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5904611a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#book has 135 pages accurately divided it as per that\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25d89de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed intro-preface and last acknowledgement page since its of no use\n",
    "documents=documents[2:134]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5984277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9e36c",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cb34071",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=RecursiveCharacterTextSplitter(chunk_size=900,chunk_overlap=90)\n",
    "text_chunks=splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a18ffcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ce117b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Introduction\n",
      "1.1 What is Machine Learning\n",
      "Machine learning is a subﬁeld of computer science that is concerned with building algorithms\n",
      "which, to be useful, rely on a collection of examples of some phenomenon. These examples\n",
      "can come from nature, be handcrafted by humans or generated by another algorithm.\n",
      "Machine learning can also be deﬁned as the process of solving a practical problem by 1)\n",
      "gathering a dataset, and 2) algorithmically building a statistical model based on that dataset.\n",
      "That statistical model is assumed to be used somehow to solve the practical problem.\n",
      "To save keystrokes, I use the terms “learning” and “machine learning” interchangeably.\n",
      "1.2 Types of Learning\n",
      "Learning can be supervised, semi-supervised, unsupervised and reinforcement.\n",
      "1.2.1 Supervised Learning\n",
      "In supervised learning1, thedataset is the collection oflabeled examples{(xi, yi)}N\n",
      "i=1.\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9b07f",
   "metadata": {},
   "source": [
    "#### Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9d6e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling embedding model\n",
    "embeddings=OllamaEmbeddings(model=\"embeddinggemma:latest\",\n",
    "                           validate_model_on_init=True,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7f608",
   "metadata": {},
   "source": [
    "#### Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a19ebb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide the list of documents to embed along with the embedding fucntion which needs to be performed on text\n",
    "vectorstore=FAISS.from_documents(text_chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54c342",
   "metadata": {},
   "source": [
    "# Perform similarity search\n",
    "query = \"explain gradient descent?\"\n",
    "docs = vectorstore.similarity_search(query, k=4)\n",
    "\n",
    "# Display the results\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaad9d2",
   "metadata": {},
   "source": [
    "#### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0de3bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045360f",
   "metadata": {},
   "source": [
    "### Query from user side------embedding-------retriver will fetch context----prompt-passing query and context retrived from retriver-----passto llm---response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c9131",
   "metadata": {},
   "source": [
    "- always decide input i.e query\n",
    "- output -i.e stroutputparser\n",
    "- llm calling model\n",
    "- chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae52895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use ten sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a969293",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8d6d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da3b77bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain\n",
    "rag_chain=(\n",
    "    {\"context\":retriver,\"question\":RunnablePassthrough()}\n",
    "    |prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2343d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=rag_chain.invoke(\"explain normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d65632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization is a preprocessing step that rescales a numerical feature to a fixed, small interval, typically [0, 1] or [–1, 1].  \n",
      "For each value x(j) of feature j, the normalized value \\(\\bar{x}(j)\\) is computed as  \n",
      "\n",
      "\\[\n",
      "\\bar{x}(j)=\\frac{x(j)-\\min(j)}{\\max(j)-\\min(j)},\n",
      "\\]\n",
      "\n",
      "where \\(\\min(j)\\) and \\(\\max(j)\\) are the smallest and largest values of that feature in the dataset.  \n",
      "By mapping all values into the same range, normalization prevents features with large magnitudes from dominating gradient‑based updates.  \n",
      "It also reduces the risk of numerical overflow or underflow when computers handle very big or very small numbers.  \n",
      "The technique is especially helpful for algorithms that rely on distance calculations or assume similarly scaled inputs.  \n",
      "While not strictly required, normalized data often leads to faster convergence during training.  \n",
      "If a feature’s natural range is, say, 350 to 1450, subtracting 350 and dividing by 1100 produces values in [0, 1].  \n",
      "Normalization is distinct from standardization (z‑score scaling), which centers data around 0 with unit variance.  \n",
      "In practice, you may try both methods and choose the one that yields better performance for your task.\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b67223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MultiDocRAG-LLMOPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
